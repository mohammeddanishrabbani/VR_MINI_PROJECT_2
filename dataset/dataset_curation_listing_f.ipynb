{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "os.chdir('/home/danish/VR_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danish/VR_PROJECT/.vr_project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set the GOOGLE_API_KEY in the environment first\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBzR5iJcm-v3j2OhnniZv5vF_fwE6v-zQY\"\n",
    "\n",
    "# Then configure the genai module with the API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# Now you can create the GenerativeModel instance\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_file = \"dataset/abo-listings/listings/metadata/listings_f.json\"\n",
    "image_metadata = pd.read_csv(\"dataset/abo-images-small/images/metadata/images.csv\")\n",
    "image_dataset_path = \"dataset/abo-images-small/images/small/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create output csv file\n",
    "output_file = f\"dataset/VQA-dataset/{listing_file.split('/')[-1].split('.')[0]}_VQA.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listing_lines(listing_file):\n",
    "    \"\"\"\n",
    "    Read the listing file and return a list of lines.\n",
    "    \"\"\"\n",
    "    with open(listing_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_product_json(product_json):\n",
    "        \"\"\"\n",
    "        Preprocess the product JSON to ensure it is in the correct format.\n",
    "        \"\"\"\n",
    "        list_of_keys_to_remove = ['main_image_id','node','other_image_id','spin_id','3dmodel_id']\n",
    "        # Convert JSON string to dictionary\n",
    "        product_dict = json.loads(product_json)\n",
    "        # Remove unnecessary keys\n",
    "        for key in list_of_keys_to_remove:\n",
    "            if key in product_dict:\n",
    "                del product_dict[key]\n",
    "        \n",
    "        # Convert dictionary to JSON string with indentation for better readability\n",
    "        return json.dumps(product_dict, indent=4)\n",
    "def prompt_for_product(product_json):\n",
    "        \"\"\"\n",
    "        Generate a prompt for the given product JSON.\n",
    "        \"\"\"\n",
    "        product_json = preprocess_product_json(product_json)\n",
    "        prompt = f\"\"\"\n",
    "        You are a QA dataset generator that creates short, factual, and human-readable question-answer pairs from Amazon product metadata and image. Each question must target a specific field from the metadata and be answerable with a **single word only**.\n",
    "\n",
    "        Below is the product metadata in structured format. Generate **5 to 10 diverse QA pairs**, where:\n",
    "        - Each question is clear and unambiguous.\n",
    "        - Each answer is strictly a **single word** (no phrases, no multi-word answers).\n",
    "        - Avoid repeating the same field.\n",
    "        - Prefer commonly relevant fields like: brand, bullet_points, color, material, product type, model name, style, fabric type, finish type, pattern, item shape, product description and color code.\n",
    "        - Questions should be such a way that they can be answered by looking at the image.\n",
    "        - The output should be in CSV format with columns: question, answer.\n",
    "\n",
    "        If a value is not meaningful or not present, skip that field. Ensure that QA pairs are diverse and aligned with the data provided.\n",
    "\n",
    "        ---\n",
    "        {product_json}\n",
    "        ---\n",
    "        \"\"\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_paths(image_ids):\n",
    "    \"\"\"\n",
    "    Get the paths of images based on their IDs.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    for image_id in image_ids:\n",
    "        image_path = image_metadata[image_metadata['image_id'] == image_id]['path'].values\n",
    "        if len(image_path) > 0:\n",
    "            if os.path.exists(f\"{image_dataset_path}/{image_path[0]}\"):\n",
    "                image_paths.append(f\"{image_path[0]}\")\n",
    "    return image_paths\n",
    "\n",
    "def generate_VQA(prompt, image_path):\n",
    "    img = Image.open(f\"{image_dataset_path}/{image_path}\")\n",
    "    img = img.convert(\"RGB\")\n",
    "    # Generate the VQA using the model\n",
    "    response = model.generate_content([prompt, img])\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = response.text\n",
    "    #read csv from the generated text\n",
    "    csv_data = pd.read_csv(StringIO(generated_text.strip(\"`\").replace(\"csv\\n\", \"\", 1).strip()))\n",
    "    return csv_data\n",
    "\n",
    "def get_VQA_for_product(product_json):\n",
    "    df = pd.DataFrame(columns=[\"image_path\",\"question\", \"answer\"])\n",
    "    list_of_image_ids = []\n",
    "    \n",
    "    prompt = prompt_for_product(product_json)\n",
    "    product_dict = json.loads(product_json)\n",
    "    if \"main_image_id\" in product_dict.keys():\n",
    "        list_of_image_ids.append(product_dict['main_image_id'])\n",
    "    if \"other_image_id\" in product_dict.keys():\n",
    "        # Check if the key exists in the dictionary\n",
    "        if isinstance(product_dict['other_image_id'], list):\n",
    "            # If it's a list, extend it to the list_of_image_ids\n",
    "            list_of_image_ids.extend(product_dict['other_image_id'])\n",
    "        else:\n",
    "            # If it's not a list, append it directly\n",
    "            list_of_image_ids.append(product_dict['other_image_id'])\n",
    "    image_paths = get_images_paths(list_of_image_ids)\n",
    "\n",
    "    # print(f\"Image paths: {image_paths}\")\n",
    "    # Generate the VQA using the model\n",
    "    if len(image_paths) == 0:\n",
    "        print(\"No images found for this product.\")\n",
    "        return df\n",
    "    for image_path in image_paths:\n",
    "        # print(f\"Generating VQA for image: {image_path}\")\n",
    "        # Generate the VQA using the model\n",
    "        csv_data = generate_VQA(prompt, image_path)\n",
    "        # Append the generated data to the dataframe\n",
    "        csv_data['image_path'] = image_path\n",
    "        df = pd.concat([df, csv_data], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/VQA-dataset/listings_f_VQA.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 300/9222 [01:04<20:01:52,  8.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 301: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 334/9222 [15:39<19:07:14,  7.74s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 335: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 351/9222 [28:18<32:18:57, 13.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved at line 350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 367/9222 [30:33<25:51:50, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 368: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 401/9222 [44:58<21:01:17,  8.58s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved at line 400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 405/9222 [45:27<21:21:48,  8.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 405: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 406/9222 [55:35<439:07:36, 179.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 406: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 407/9222 [1:05:53<750:09:35, 306.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 408: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 409/9222 [1:15:59<746:31:13, 304.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 409: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 419/9222 [1:27:04<90:02:49, 36.82s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 419: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 423/9222 [1:38:00<191:29:36, 78.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 423: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 450/9222 [2:07:52<23:46:11,  9.76s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 450: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 451/9222 [2:33:19<1100:42:07, 451.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved at line 450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 466/9222 [2:36:02<40:42:22, 16.74s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 466: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 471/9222 [2:47:00<132:02:15, 54.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 471: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 472/9222 [3:12:27<1205:12:48, 495.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 472: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 488/9222 [3:24:47<32:15:49, 13.30s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 488: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 501/9222 [3:52:03<37:50:23, 15.62s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved at line 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 510/9222 [3:53:13<20:42:35,  8.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 510: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 520/9222 [4:04:35<42:11:34, 17.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 520: 429 Resource has been exhausted (e.g. check quota).\n",
      "Resource has been exhausted. Please try again later.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 520/9222 [4:29:45<171:35:37, 70.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 429 Resource has been exhausted (e.g. check quota).\n",
      "VQA dataset saved to dataset/VQA-dataset/listings_f_VQA.csv\n",
      "Progress saved at line 520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "print(output_file)\n",
    "if not os.path.exists(output_file):\n",
    "    print(f\"Output file {output_file} does not exist. Creating a new one.\")\n",
    "    output_df = pd.DataFrame(columns=[\"image_path\",\"question\", \"answer\"])\n",
    "else:\n",
    "    output_df = pd.read_csv(output_file)\n",
    "\n",
    "\n",
    "lines = get_listing_lines(listing_file)\n",
    "\n",
    "# subset_lines = lines[556:]  # to process a subset of lines\n",
    "start_index = 292\n",
    "try:\n",
    "    for i in tqdm(range(start_index, len(lines)), initial=start_index, total=len(lines)):\n",
    "        line = lines[i]\n",
    "        if \"\\\"en_\" not in line:\n",
    "            continue\n",
    "        try:\n",
    "            df = get_VQA_for_product(line)\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line {i}: {e}\")\n",
    "            if \"Resource has been exhausted\" in str(e):\n",
    "                print(\"Resource has been exhausted. Please try again later.\")\n",
    "                time.sleep(600)\n",
    "                try:\n",
    "                    df = get_VQA_for_product(line)\n",
    "                except Exception as e:\n",
    "                    time.sleep(900)\n",
    "                    df = get_VQA_for_product(line)\n",
    "            else:\n",
    "                continue\n",
    "        output_df = pd.concat([output_df, df], ignore_index=True)\n",
    "        if i % 50 == 0:\n",
    "            # Save the output DataFrame to a CSV file every 50 iterations\n",
    "            output_df.to_csv(output_file, index=False)\n",
    "            print(f\"Progress saved at line {i}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    # Handle the exception as needed\n",
    "    pass\n",
    "# Save the output DataFrame to a CSV file\n",
    "output_df.to_csv(output_file, index=False)\n",
    "print(f\"VQA dataset saved to {output_file}\") \n",
    "print(f\"Progress saved at line {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
