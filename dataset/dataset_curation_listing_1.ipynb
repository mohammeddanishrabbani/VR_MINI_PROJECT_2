{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"E2JGVI-nahwU","executionInfo":{"status":"ok","timestamp":1746977599066,"user_tz":-330,"elapsed":296,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import json\n","from PIL import Image\n","from io import StringIO"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"H5XcmgUFahwZ","executionInfo":{"status":"ok","timestamp":1746977601149,"user_tz":-330,"elapsed":1281,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}}},"outputs":[],"source":["import os\n","import google.generativeai as genai\n","\n","# Set the GOOGLE_API_KEY in the environment first\n","os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyA-R3wvqFinEGCJG73sCxfI8qDaQNuROvg\"\n","\n","# Then configure the genai module with the API key\n","genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n","\n","# Now you can create the GenerativeModel instance\n","model = genai.GenerativeModel('gemini-2.0-flash-001')"]},{"cell_type":"code","source":["!wget https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8fcyS-oFcoIZ","executionInfo":{"status":"ok","timestamp":1746977695828,"user_tz":-330,"elapsed":93351,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}},"outputId":"dde86b93-d197-4a94-83dd-a9f1aa70441e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-05-11 15:33:21--  https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n","Resolving amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)... 3.5.6.11, 52.216.53.241, 3.5.27.111, ...\n","Connecting to amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)|3.5.6.11|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3253381120 (3.0G) [application/x-tar]\n","Saving to: ‘abo-images-small.tar’\n","\n","abo-images-small.ta 100%[===================>]   3.03G  36.9MB/s    in 93s     \n","\n","2025-05-11 15:34:54 (33.4 MB/s) - ‘abo-images-small.tar’ saved [3253381120/3253381120]\n","\n"]}]},{"cell_type":"code","source":["!tar -xf abo-images-small.tar"],"metadata":{"id":"uPDt7eapcsQG","executionInfo":{"status":"ok","timestamp":1746977726101,"user_tz":-330,"elapsed":30269,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"4VGQjdANahwb","executionInfo":{"status":"ok","timestamp":1746977758835,"user_tz":-330,"elapsed":2286,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}}},"outputs":[],"source":["listing_file = \"/content/drive/MyDrive/abo-listings/listings/metadata/listings_1.json.gz\"\n","image_metadata = pd.read_csv(\"/content/drive/MyDrive/images/metadata/images.csv\")\n","image_dataset_path = \"/content/images/small\""]},{"cell_type":"code","execution_count":7,"metadata":{"id":"p3IwpQDWahwc","executionInfo":{"status":"ok","timestamp":1746977758850,"user_tz":-330,"elapsed":10,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}}},"outputs":[],"source":["#create output csv file\n","output_file = f\"/content/drive/MyDrive/images/{listing_file.split('/')[-1].split('.')[0]}_VQA.csv\""]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzrW3VEOcCxY","executionInfo":{"status":"ok","timestamp":1746977756554,"user_tz":-330,"elapsed":30462,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}},"outputId":"da709ba9-e813-4432-f3c1-896bf8849abd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"XhN5x4tfahwd","executionInfo":{"status":"ok","timestamp":1746977758863,"user_tz":-330,"elapsed":8,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}}},"outputs":[],"source":["# def get_listing_lines(listing_file):\n","#     \"\"\"\n","#     Read the listing file and return a list of lines.\n","#     \"\"\"\n","#     with open(listing_file, 'r') as file:\n","#         lines = file.readlines()\n","#     return lines\n","import gzip\n","\n","def get_listing_lines(listing_file):\n","    \"\"\"\n","    Read the listing file and return a list of lines.\n","    Handles gzip compression if the file name ends with '.gz'.\n","    \"\"\"\n","    if listing_file.endswith('.gz'):\n","        with gzip.open(listing_file, 'rt', encoding='utf-8') as file:\n","            lines = file.readlines()\n","    else:\n","        with open(listing_file, 'r', encoding='utf-8') as file:\n","            lines = file.readlines()\n","    return lines\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"kZVREXfiahwe","executionInfo":{"status":"ok","timestamp":1746977758888,"user_tz":-330,"elapsed":17,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}}},"outputs":[],"source":["def preprocess_product_json(product_json):\n","        \"\"\"\n","        Preprocess the product JSON to ensure it is in the correct format.\n","        \"\"\"\n","        list_of_keys_to_remove = ['main_image_id','node','other_image_id','spin_id','3dmodel_id']\n","        # Convert JSON string to dictionary\n","        product_dict = json.loads(product_json)\n","        # Remove unnecessary keys\n","        for key in list_of_keys_to_remove:\n","            if key in product_dict:\n","                del product_dict[key]\n","\n","        # Convert dictionary to JSON string with indentation for better readability\n","        return json.dumps(product_dict, indent=4)\n","def prompt_for_product(product_json):\n","        \"\"\"\n","        Generate a prompt for the given product JSON.\n","        \"\"\"\n","        preprocess_product_json(product_json)\n","        prompt = f\"\"\"\n","        You are a QA dataset generator that creates short, factual, and human-readable question-answer pairs from Amazon product metadata and image. Each question must target a specific field from the metadata and be answerable with a *single word only*.\n","\n","        Below is the product metadata in structured format. Generate *5 to 10 diverse QA pairs*, where:\n","        - Each question is clear and unambiguous.\n","        - Each answer is strictly a *single word* (no phrases, no multi-word answers).\n","        - Avoid repeating the same field.\n","        - Prefer commonly relevant fields like: brand, bullet_points, color, material, product type, model name, style, fabric type, finish type, pattern, item shape, product description and color code.\n","        - Questions should be such a way that they can be answered by looking at the image.\n","        - The output should be in CSV format with columns: question, answer.\n","\n","        If a value is not meaningful or not present, skip that field. Ensure that QA pairs are diverse and aligned with the data provided.\n","\n","        ---\n","        {product_json}\n","        ---\n","        \"\"\"\n","\n","        return prompt"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"yws965X6ahwf","executionInfo":{"status":"ok","timestamp":1746977758918,"user_tz":-330,"elapsed":25,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}}},"outputs":[],"source":["def get_images_paths(image_ids):\n","    \"\"\"\n","    Get the paths of images based on their IDs.\n","    \"\"\"\n","    image_paths = []\n","    for image_id in image_ids:\n","        image_path = image_metadata[image_metadata['image_id'] == image_id]['path'].values\n","        if len(image_path) > 0:\n","            if os.path.exists(f\"{image_dataset_path}/{image_path[0]}\"):\n","                image_paths.append(f\"{image_path[0]}\")\n","    return image_paths\n","\n","def generate_VQA(prompt, image_path):\n","    img = Image.open(f\"{image_dataset_path}/{image_path}\")\n","    img = img.convert(\"RGB\")\n","    # Generate the VQA using the model\n","    response = model.generate_content([prompt, img])\n","    # Extract the generated text from the response\n","    generated_text = response.text\n","    #read csv from the generated text\n","    csv_data = pd.read_csv(StringIO(generated_text.strip(\"`\").replace(\"csv\\n\", \"\", 1).strip()))\n","    return csv_data\n","\n","def get_VQA_for_product(product_json):\n","    df = pd.DataFrame(columns=[\"image_path\",\"question\", \"answer\"])\n","    list_of_image_ids = []\n","\n","    prompt = prompt_for_product(product_json)\n","    product_dict = json.loads(product_json)\n","    list_of_image_ids.append(product_dict['main_image_id'])\n","    if \"other_image_id\" in product_dict.keys():\n","        # Check if the key exists in the dictionary\n","        if isinstance(product_dict['other_image_id'], list):\n","            # If it's a list, extend it to the list_of_image_ids\n","            list_of_image_ids.extend(product_dict['other_image_id'])\n","        else:\n","            # If it's not a list, append it directly\n","            list_of_image_ids.append(product_dict['other_image_id'])\n","    image_paths = get_images_paths(list_of_image_ids)\n","\n","    print(f\"Image paths: {image_paths}\")\n","    # Generate the VQA using the model\n","    for image_path in image_paths:\n","        print(f\"Generating VQA for image: {image_path}\")\n","        # Generate the VQA using the model\n","        csv_data = generate_VQA(prompt, image_path)\n","        import time\n","        time.sleep(2)\n","        # Append the generated data to the dataframe\n","        csv_data['image_path'] = image_path\n","        df = pd.concat([df, csv_data], ignore_index=True)\n","    return df\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":783},"id":"CaoFHmOqahwg","outputId":"3f619f60-a518-4815-f4bc-24b12f8bd010","executionInfo":{"status":"ok","timestamp":1746977890105,"user_tz":-330,"elapsed":131188,"user":{"displayName":"Anukriti Singh","userId":"12373033605025145532"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/8100 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Image paths: ['9c/9ca0d27d.jpg', 'ee/ee8ee952.jpg', '49/49b1b22b.jpg', '66/66f3a68c.jpg', 'b2/b2ff8632.jpg']\n","Generating VQA for image: 9c/9ca0d27d.jpg\n","Generating VQA for image: ee/ee8ee952.jpg\n","Generating VQA for image: 49/49b1b22b.jpg\n","Generating VQA for image: 66/66f3a68c.jpg\n","Generating VQA for image: b2/b2ff8632.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 1/8100 [00:22<51:24:58, 22.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Image paths: ['a1/a1b1d8d5.jpg', '2f/2f357c87.jpg', '85/85192c70.jpg']\n","Generating VQA for image: a1/a1b1d8d5.jpg\n","Generating VQA for image: 2f/2f357c87.jpg\n","Generating VQA for image: 85/85192c70.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 3/8100 [00:35<23:44:37, 10.56s/it]"]},{"output_type":"stream","name":"stdout","text":["Image paths: ['c9/c9ce8c90.jpg', 'ee/ee8ee952.jpg', '49/49b1b22b.jpg', '66/66f3a68c.jpg', '10/10146912.jpg']\n","Generating VQA for image: c9/c9ce8c90.jpg\n","Generating VQA for image: ee/ee8ee952.jpg\n","Generating VQA for image: 49/49b1b22b.jpg\n","Generating VQA for image: 66/66f3a68c.jpg\n","Generating VQA for image: 10/10146912.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 4/8100 [00:56<31:54:54, 14.19s/it]"]},{"output_type":"stream","name":"stdout","text":["Image paths: ['ca/ca155f75.jpg', 'c0/c0229ce0.jpg', '9e/9eb555dd.jpg']\n","Generating VQA for image: ca/ca155f75.jpg\n","Generating VQA for image: c0/c0229ce0.jpg\n","Generating VQA for image: 9e/9eb555dd.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 5/8100 [01:08<30:15:49, 13.46s/it]"]},{"output_type":"stream","name":"stdout","text":["Image paths: ['dd/dd8d8182.jpg', '2f/2f357c87.jpg', '85/85192c70.jpg']\n","Generating VQA for image: dd/dd8d8182.jpg\n","Generating VQA for image: 2f/2f357c87.jpg\n","Generating VQA for image: 85/85192c70.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 6/8100 [01:20<29:26:39, 13.10s/it]"]},{"output_type":"stream","name":"stdout","text":["Image paths: ['8e/8e7480df.jpg', 'ee/ee8ee952.jpg', '49/49b1b22b.jpg', '66/66f3a68c.jpg', '89/8980e982.jpg']\n","Generating VQA for image: 8e/8e7480df.jpg\n","Generating VQA for image: ee/ee8ee952.jpg\n","Generating VQA for image: 49/49b1b22b.jpg\n","Generating VQA for image: 66/66f3a68c.jpg\n","Generating VQA for image: 89/8980e982.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 7/8100 [01:40<34:04:15, 15.16s/it]"]},{"output_type":"stream","name":"stdout","text":["Image paths: ['7f/7f29db56.jpg', '1f/1f3d7ced.jpg', 'a8/a82dcccc.jpg', '8a/8a4fc2da.jpg', 'f3/f32a9034.jpg', '88/88dbb009.jpg']\n","Generating VQA for image: 7f/7f29db56.jpg\n","Generating VQA for image: 1f/1f3d7ced.jpg\n","Generating VQA for image: a8/a82dcccc.jpg\n","Generating VQA for image: 8a/8a4fc2da.jpg\n","Generating VQA for image: f3/f32a9034.jpg\n","Generating VQA for image: 88/88dbb009.jpg\n"]},{"output_type":"stream","name":"stderr","text":["\r  0%|          | 8/8100 [02:04<40:03:56, 17.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Image paths: ['b4/b4045219.jpg', '6e/6ef274c7.jpg', '2c/2c7b280e.jpg', '5a/5ac3ea23.jpg', '8c/8c48826c.jpg', 'b2/b2d6f4d8.jpg', '13/13c410d3.jpg']\n","Generating VQA for image: b4/b4045219.jpg\n","Generating VQA for image: 6e/6ef274c7.jpg\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tornado.access:429 POST /v1beta/models/gemini-2.0-flash-001:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 730.85ms\n","\r  0%|          | 8/8100 [02:09<36:19:54, 16.16s/it]"]},{"output_type":"stream","name":"stdout","text":["An error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-001:generateContent?%24alt=json%3Benum-encoding%3Dint: Resource has been exhausted (e.g. check quota).\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["from tqdm import tqdm\n","\n","# Prepare existing CSV\n","if not os.path.exists(output_file):\n","    output_df = pd.DataFrame(columns=[\"image_path\", \"question\", \"answer\"])\n","else:\n","    output_df = pd.read_csv(output_file)\n","\n","# Track processed image_ids\n","processed_ids = set(output_df['image_path'].unique())\n","\n","# Load metadata lines\n","lines = get_listing_lines(listing_file)\n","subset_lines = lines[900:9000]\n","\n","try:\n","    for line in tqdm(subset_lines):\n","        if \"\\\"en_\" not in line:\n","            continue\n","\n","        try:\n","            product_dict = json.loads(line)\n","            main_image_id = product_dict.get('main_image_id', None)\n","            if not main_image_id or main_image_id in processed_ids:\n","                continue\n","        except Exception:\n","            continue\n","\n","        df = get_VQA_for_product(line)\n","        output_df = pd.concat([output_df, df], ignore_index=True)\n","\n","        # Update processed_ids dynamically to reduce memory usage\n","        processed_ids.update(df['image_path'].unique())\n","\n","        # Save incrementally to avoid data loss\n","        output_df.to_csv(output_file, index=False)\n","\n","except Exception as e:\n","    print(f\"An error occurred: {e}\")\n","    pass\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwElG9Myahwh"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}