{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import StringIO\n",
    "os.chdir('/home/danish/VR_PROJECT')\n",
    "os.environ[\"model\"] = \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danish/VR_PROJECT/.vr_project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Set the GOOGLE_API_KEY in the environment first\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBsPNOTz8nJWYyk-ksbRjufDZTFCrkqnms\"\n",
    "\n",
    "# Then configure the genai module with the API key\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# Now you can create the GenerativeModel instance\n",
    "model = genai.GenerativeModel('gemini-2.0-flash-001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_file = \"dataset/abo-listings/listings/metadata/listings_4.json\"\n",
    "image_metadata = pd.read_csv(\"dataset/abo-images-small/images/metadata/images.csv\")\n",
    "image_dataset_path = \"dataset/abo-images-small/images/small\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create output csv file\n",
    "output_file = f\"dataset/VQA-dataset/{listing_file.split('/')[-1].split('.')[0]}_VQA.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listing_lines(listing_file):\n",
    "    \"\"\"\n",
    "    Read the listing file and return a list of lines.\n",
    "    \"\"\"\n",
    "    with open(listing_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_product_json(product_json):\n",
    "        \"\"\"\n",
    "        Preprocess the product JSON to ensure it is in the correct format.\n",
    "        \"\"\"\n",
    "        list_of_keys_to_remove = ['main_image_id','node','other_image_id','spin_id','3dmodel_id']\n",
    "        # Convert JSON string to dictionary\n",
    "        product_dict = json.loads(product_json)\n",
    "        # Remove unnecessary keys\n",
    "        for key in list_of_keys_to_remove:\n",
    "            if key in product_dict:\n",
    "                del product_dict[key]\n",
    "        \n",
    "        # Convert dictionary to JSON string with indentation for better readability\n",
    "        return json.dumps(product_dict, indent=4)\n",
    "def prompt_for_product(product_json):\n",
    "        \"\"\"\n",
    "        Generate a prompt for the given product JSON.\n",
    "        \"\"\"\n",
    "        preprocess_product_json(product_json)\n",
    "        prompt = f\"\"\"\n",
    "        You are a QA dataset generator that creates short, factual, and human-readable question-answer pairs from Amazon product metadata and image. Each question must target a specific field from the metadata and be answerable with a **single word only**.\n",
    "\n",
    "        Below is the product metadata in structured format. Generate **5 to 10 diverse QA pairs**, where:\n",
    "        - Each question is clear and unambiguous.\n",
    "        - Each answer is strictly a **single word** (no phrases, no multi-word answers).\n",
    "        - Avoid repeating the same field.\n",
    "        - Prefer commonly relevant fields like: brand, bullet_points, color, material, product type, model name, style, fabric type, finish type, pattern, item shape, product description and color code.\n",
    "        - Questions should be such a way that they can be answered by looking at the image.\n",
    "        - The output should be in CSV format with columns: question, answer.\n",
    "\n",
    "        If a value is not meaningful or not present, skip that field. Ensure that QA pairs are diverse and aligned with the data provided.\n",
    "\n",
    "        ---\n",
    "        {product_json}\n",
    "        ---\n",
    "        \"\"\"\n",
    "        \n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "def generate_from_ollama(prompt, image_path):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "    base64_image = base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"llava-phi3\",\n",
    "        \"prompt\": prompt,\n",
    "        \"images\": [base64_image],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(\"http://localhost:11434/api/generate\", json=payload)\n",
    "    return response.json()['response']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images_paths(image_ids):\n",
    "    \"\"\"\n",
    "    Get the paths of images based on their IDs.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    for image_id in image_ids:\n",
    "        image_path = image_metadata[image_metadata['image_id'] == image_id]['path'].values\n",
    "        if len(image_path) > 0:\n",
    "            if os.path.exists(f\"{image_dataset_path}/{image_path[0]}\"):\n",
    "                image_paths.append(f\"{image_path[0]}\")\n",
    "    return image_paths\n",
    "\n",
    "\n",
    "def generate_VQA(prompt, image_path):\n",
    "    if os.environ[\"model\"] == \"local\":\n",
    "        # Use the local model for VQA\n",
    "        response = generate_from_ollama(prompt, image_path)\n",
    "        print(response)\n",
    "        # Extract the generated text from the response\n",
    "        generated_text = response['text']\n",
    "    else:\n",
    "        img = Image.open(f\"{image_dataset_path}/{image_path}\")\n",
    "        img = img.convert(\"RGB\")\n",
    "        # Generate the VQA using the model\n",
    "        response = model.generate_content([prompt, img])\n",
    "        # Extract the generated text from the response\n",
    "        generated_text = response.text\n",
    "    #read csv from the generated text\n",
    "    csv_data = pd.read_csv(StringIO(generated_text.strip(\"`\").replace(\"csv\\n\", \"\", 1).strip()))\n",
    "    return csv_data\n",
    "\n",
    "def get_VQA_for_product(product_json):\n",
    "    df = pd.DataFrame(columns=[\"image_path\",\"question\", \"answer\"])\n",
    "    list_of_image_ids = []\n",
    "    \n",
    "    prompt = prompt_for_product(product_json)\n",
    "    product_dict = json.loads(product_json)\n",
    "    if \"main_image_id\" in product_dict.keys():\n",
    "        list_of_image_ids.append(product_dict['main_image_id'])\n",
    "    if \"other_image_id\" in product_dict.keys():\n",
    "        # Check if the key exists in the dictionary\n",
    "        if isinstance(product_dict['other_image_id'], list):\n",
    "            # If it's a list, extend it to the list_of_image_ids\n",
    "            list_of_image_ids.extend(product_dict['other_image_id'])\n",
    "        else:\n",
    "            # If it's not a list, append it directly\n",
    "            list_of_image_ids.append(product_dict['other_image_id'])\n",
    "    image_paths = get_images_paths(list_of_image_ids)\n",
    "\n",
    "    # print(f\"Image paths: {image_paths}\")\n",
    "    # Generate the VQA using the model\n",
    "    if len(image_paths) == 0:\n",
    "        print(\"No images found for this product.\")\n",
    "        return df\n",
    "    for image_path in image_paths:\n",
    "        # print(f\"Generating VQA for image: {image_path}\")\n",
    "        # Generate the VQA using the model\n",
    "        csv_data = generate_VQA(prompt, image_path)\n",
    "        # Append the generated data to the dataframe\n",
    "        csv_data['image_path'] = image_path\n",
    "        df = pd.concat([df, csv_data], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/VQA-dataset/listings_4_VQA.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing line 654: [Errno 2] No such file or directory: '2c/2c0416de.jpg'\n",
      "VQA dataset saved to dataset/VQA-dataset/listings_4_VQA.csv\n",
      "Progress saved at line 654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "print(output_file)\n",
    "if not os.path.exists(output_file):\n",
    "    print(f\"Output file {output_file} does not exist. Creating a new one.\")\n",
    "    output_df = pd.DataFrame(columns=[\"image_path\",\"question\", \"answer\"])\n",
    "else:\n",
    "    output_df = pd.read_csv(output_file)\n",
    "\n",
    "\n",
    "lines = get_listing_lines(listing_file)\n",
    "\n",
    "# subset_lines = lines[556:]  # to process a subset of lines\n",
    "start_index = 654\n",
    "try:\n",
    "    # for i in tqdm(range(start_index, len(lines)), initial=start_index, total=len(lines)):\n",
    "    for i in tqdm(range(start_index, start_index +1)):\n",
    "        line = lines[i]\n",
    "        if \"\\\"en_\" not in line:\n",
    "            continue\n",
    "        try:\n",
    "            df = get_VQA_for_product(line)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing line {i}: {e}\")\n",
    "            if \"Resource has been exhausted\" in str(e):\n",
    "                print(\"Resource has been exhausted. Please try again later.\")\n",
    "                time.sleep(60)\n",
    "                df = get_VQA_for_product(line)\n",
    "            else:\n",
    "                continue\n",
    "        output_df = pd.concat([output_df, df], ignore_index=True)\n",
    "        if i % 50 == 0:\n",
    "            # Save the output DataFrame to a CSV file every 50 iterations\n",
    "            output_df.to_csv(output_file, index=False)\n",
    "            print(f\"Progress saved at line {i}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    # Handle the exception as needed\n",
    "    pass\n",
    "# Save the output DataFrame to a CSV file\n",
    "output_df.to_csv(output_file, index=False)\n",
    "print(f\"VQA dataset saved to {output_file}\") \n",
    "print(f\"Progress saved at line {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".vr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
