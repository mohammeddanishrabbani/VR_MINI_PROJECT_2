{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8WdQmCTyYt5d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n",
        "!tar -xf abo-images-small.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKP5yWZ6Y-Iz",
        "outputId": "b0c4d0bc-1d05-4151-c952-d750e3557c5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-14 07:49:16--  https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n",
            "Resolving amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)... 52.217.96.252, 54.231.235.25, 52.217.75.172, ...\n",
            "Connecting to amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)|52.217.96.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3253381120 (3.0G) [application/x-tar]\n",
            "Saving to: ‘abo-images-small.tar’\n",
            "\n",
            "abo-images-small.ta 100%[===================>]   3.03G  41.7MB/s    in 85s     \n",
            "\n",
            "2025-05-14 07:50:42 (36.3 MB/s) - ‘abo-images-small.tar’ saved [3253381120/3253381120]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvYfeUwRZEz0",
        "outputId": "ef2c25c6-4e0f-4c50-fdc1-c2daa70b7f16"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 1: Install Required Packages\n",
        "# =======================\n",
        "!pip install -q bitsandbytes accelerate transformers --quiet\n"
      ],
      "metadata": {
        "id": "fyPQwKf8ZHa_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "938c2d6c-7437-472c-b563-79106dcea4e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 2: Python Script for VQA Inference\n",
        "# =======================\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "# !rm -rf ~/.cache/huggingface/\n",
        "# !rm -rf /content/unsloth_compiled_cache\n",
        "\n",
        "# Constants for script-based compatibility\n",
        "class Args:\n",
        "    image_dir = \"/content/images/small/\"\n",
        "    csv_path = \"/content/drive/MyDrive/images/VQA_dataset_test/merged_listings_test.csv\"\n",
        "    model_name = \"unsloth/Qwen2-VL-7B-Instruct-unsloth-bnb-4bit\"\n",
        "args = Args()\n",
        "\n",
        "def main():\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('--image_dir', type=str, required=True, help='Path to image folder')\n",
        "    # parser.add_argument('--csv_path', type=str, required=True, help='Path to image-metadata CSV')\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Load metadata CSV\n",
        "    df = pd.read_csv(args.csv_path)\n",
        "    df = df[:5000]  # Sample 10 rows for testing\n",
        "\n",
        "    # Load model and processor, move model to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    args.model_name,\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        "    )\n",
        "    FastVisionModel.for_inference(model)\n",
        "\n",
        "\n",
        "    generated_answers = []\n",
        "    for idx, row in tqdm(df.iterrows(), total=1000):\n",
        "        image_path = f\"{args.image_dir}/{row['image_path']}\"\n",
        "        question = f\"{str(row['question'])}. Answer in one word.\"\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            messages = [\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"image\"},\n",
        "                        {\"type\": \"text\", \"text\": question}\n",
        "                    ]}\n",
        "                ]\n",
        "            input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
        "            inputs = tokenizer(\n",
        "                    image,\n",
        "                    input_text,\n",
        "                    add_special_tokens = False,\n",
        "                    return_tensors = \"pt\",\n",
        "                ).to(device)\n",
        "            generated_answer = model.generate(**inputs, max_length=2000)\n",
        "            answer = tokenizer.decode(generated_answer[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {e}\")\n",
        "            answer = \"error\"\n",
        "        # Ensure answer is one word and in English (basic post-processing)\n",
        "        answer = str(answer).split('assistant\\n')[-1].lower()\n",
        "        generated_answers.append(answer)\n",
        "\n",
        "    df[\"generated_answer\"] = generated_answers\n",
        "    df.to_csv(\"results_QWEN7B.csv\", index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfXEAUWcZ7jT",
        "outputId": "b8ff6330-4235-4e0b-d949-24dc30904d9c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.2: Fast Qwen2 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1884it [11:08,  3.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image /content/images/small//4f/4f7c30f0.jpg: height:27 and width:256 must be larger than factor:28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2062it [13:57,  4.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image /content/images/small//5d/5d626cd9.jpg: height:27 and width:256 must be larger than factor:28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2325it [15:11,  4.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image /content/images/small//c0/c081c885.jpg: height:22 and width:256 must be larger than factor:28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4233it [26:14,  1.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image /content/images/small//e1/e167d5c9.jpg: height:22 and width:256 must be larger than factor:28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5000it [29:49,  2.79it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "pPVZMrx0iLcq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}