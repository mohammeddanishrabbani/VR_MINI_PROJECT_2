{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8WdQmCTyYt5d"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n",
        "!tar -xf abo-images-small.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKP5yWZ6Y-Iz",
        "outputId": "218b92e6-cc52-42b2-fb0f-29ecc6c1a124"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-14 06:03:56--  https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n",
            "Resolving amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)... 52.216.215.121, 16.15.217.115, 52.217.73.84, ...\n",
            "Connecting to amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)|52.216.215.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3253381120 (3.0G) [application/x-tar]\n",
            "Saving to: ‘abo-images-small.tar’\n",
            "\n",
            "abo-images-small.ta 100%[===================>]   3.03G  46.9MB/s    in 89s     \n",
            "\n",
            "2025-05-14 06:05:25 (35.0 MB/s) - ‘abo-images-small.tar’ saved [3253381120/3253381120]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvYfeUwRZEz0",
        "outputId": "fc168bec-e5ab-4d5a-bb39-f6b3754d677e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 1: Install Required Packages\n",
        "# =======================\n",
        "!pip install -q bitsandbytes accelerate transformers --quiet\n"
      ],
      "metadata": {
        "id": "fyPQwKf8ZHa_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 2: Python Script for VQA Inference\n",
        "# =======================\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# Constants for script-based compatibility\n",
        "class Args:\n",
        "    image_dir = \"/content/images/small/\"\n",
        "    csv_path = \"/content/drive/MyDrive/images/VQA_dataset_test/merged_listings_test.csv\"\n",
        "    model_name = \"unsloth/Qwen2.5-7B\"\n",
        "args = Args()\n",
        "\n",
        "def main():\n",
        "    # parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument('--image_dir', type=str, required=True, help='Path to image folder')\n",
        "    # parser.add_argument('--csv_path', type=str, required=True, help='Path to image-metadata CSV')\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    # Load metadata CSV\n",
        "    df = pd.read_csv(args.csv_path)\n",
        "    df = df[:5000]  # Sample 10 rows for testing\n",
        "\n",
        "    # Load model and processor, move model to GPU if available\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        "    )\n",
        "    FastVisionModel.for_inference(model)\n",
        "\n",
        "\n",
        "    generated_answers = []\n",
        "    for idx, row in tqdm(df.iterrows(), total=10):\n",
        "        image_path = f\"{args.image_dir}/{row['image_path']}\"\n",
        "        question = f\"{str(row['question'])}. Answer in one word.\"\n",
        "        try:\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            messages = [\n",
        "                    {\"role\": \"user\", \"content\": [\n",
        "                        {\"type\": \"image\"},\n",
        "                        {\"type\": \"text\", \"text\": question}\n",
        "                    ]}\n",
        "                ]\n",
        "            input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
        "            inputs = tokenizer(\n",
        "                    image,\n",
        "                    input_text,\n",
        "                    add_special_tokens = False,\n",
        "                    return_tensors = \"pt\",\n",
        "                ).to(device)\n",
        "            generated_answer = model.generate(**inputs, max_length=300)\n",
        "\n",
        "            answer = tokenizer.decode(generated_answer[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_path}: {e}\")\n",
        "            answer = \"error\"\n",
        "        # Ensure answer is one word and in English (basic post-processing)\n",
        "        answer = str(answer).split('assistant\\n')[-1].lower()\n",
        "        generated_answers.append(answer)\n",
        "\n",
        "    df[\"generated_answer\"] = generated_answers\n",
        "    df.to_csv(\"results_qwen7B.csv\", index=False)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfXEAUWcZ7jT",
        "outputId": "37604cab-078c-4f5d-8ddb-a563cdd07390"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.5.2: Fast Qwen2 patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1884it [08:22,  4.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image /content/images/small//4f/4f7c30f0.jpg: height:27 and width:256 must be larger than factor:28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2062it [09:08,  4.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image /content/images/small//5d/5d626cd9.jpg: height:27 and width:256 must be larger than factor:28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2325it [10:16,  3.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image /content/images/small//c0/c081c885.jpg: height:22 and width:256 must be larger than factor:28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4233it [18:42,  3.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image /content/images/small//e1/e167d5c9.jpg: height:22 and width:256 must be larger than factor:28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5000it [22:06,  3.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pPVZMrx0iLcq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}