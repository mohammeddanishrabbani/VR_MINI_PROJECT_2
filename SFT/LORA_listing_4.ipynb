{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXjOiIVa-wYc",
        "outputId": "69db5919-6871-4493-a6ef-a79cd497b44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-13 04:27:04--  https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n",
            "Resolving amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)... 54.231.227.113, 16.182.100.225, 52.217.160.57, ...\n",
            "Connecting to amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)|54.231.227.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3253381120 (3.0G) [application/x-tar]\n",
            "Saving to: ‘abo-images-small.tar’\n",
            "\n",
            "abo-images-small.ta 100%[===================>]   3.03G  14.0MB/s    in 3m 39s  \n",
            "\n",
            "2025-05-13 04:30:44 (14.1 MB/s) - ‘abo-images-small.tar’ saved [3253381120/3253381120]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xf abo-images-small.tar"
      ],
      "metadata": {
        "id": "ZT4dix41-1wt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tId_gY5bmuP",
        "outputId": "0957d239-6746-46fe-9df1-544956e4c8b0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers peft accelerate datasets bert-score --quiet"
      ],
      "metadata": {
        "id": "KThwWljS5wwI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc2f431-cd50-41cb-ce57-f7991dbb8c81"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from transformers import BlipProcessor, BlipForQuestionAnswering, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset, Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "1vL7Z0iwDPxo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========= CONFIG =========\n",
        "listing = \"4\"\n",
        "csv_path = '/content/drive/MyDrive/images/VQA_dataset_train/listings_4_VQA_train.csv'  # CSV must contain: image_path, question, one_word_answer\n",
        "#get length of csv file\n",
        "# csv_path = f\"/content/drive/MyDrive/images/VQA-dataset-train/listings_{listing}_VQA_train.csv\"\n",
        "f = open(csv_path, \"r\")\n",
        "lines = f.readlines()\n",
        "f.close()\n",
        "\n",
        "model_name = \"Salesforce/blip-vqa-base\"\n",
        "output_dir = f\"/content/drive/MyDrive/images/lora_on_listing_{listing}\"\n",
        "images_root = \"/content/images/small\"\n",
        "batch_size = 16\n",
        "max_steps = (len(lines)-1)//batch_size\n",
        "print(f\"Steps per epoch: {max_steps}\")\n",
        "checkpoint_path = '/content/drive/MyDrive/images/lora_on_listing_0/checkpoint-11000'\n",
        "# ===========================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "del lines"
      ],
      "metadata": {
        "id": "27wAgGakbbFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d705b32-66d0-40f3-9fc4-1938dac55cf4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch: 9693\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class VQALazyIterableDataset(IterableDataset):\n",
        "    def __init__(self, dataset_stream, processor):\n",
        "        self.dataset_stream = dataset_stream\n",
        "        self.processor = processor\n",
        "        self.images_root = \"/content/images/small\"\n",
        "\n",
        "    def preprocess(self, example):\n",
        "        try:\n",
        "\n",
        "            image = Image.open(f\"{self.images_root}/{example['image_path']}\").convert(\"RGB\")\n",
        "\n",
        "            inputs = self.processor(\n",
        "                image,\n",
        "                example[\"question\"],\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=50,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\"\n",
        "            )\n",
        "\n",
        "            labels = self.processor.tokenizer(\n",
        "                text=example[\"answer\"],\n",
        "                return_tensors=\"pt\",\n",
        "                max_length=20,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\"\n",
        "            ).input_ids\n",
        "\n",
        "            return {\n",
        "                \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
        "                \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "                \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "                \"labels\": labels.squeeze(0)\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping example due to error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def __iter__(self):\n",
        "        for example in self.dataset_stream:\n",
        "            processed = self.preprocess(example)\n",
        "            if processed:\n",
        "                yield processed\n"
      ],
      "metadata": {
        "id": "R1497J8OjHcj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream CSV dataset\n",
        "# streamed_dataset = load_dataset(\"csv\", data_files=csv_path, split=\"train\", streaming=True)\n",
        "df = pd.read_csv(csv_path)\n",
        "streamed_dataset = Dataset.from_pandas(df).to_iterable_dataset()\n",
        "\n",
        "streamed_dataset = streamed_dataset.filter(\n",
        "    lambda x: x[\"image_path\"] and x[\"question\"] and x[\"answer\"]\n",
        ")\n",
        "\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Wrap in PyTorch Dataset\n",
        "torch_dataset = VQALazyIterableDataset(streamed_dataset, processor)\n",
        "\n",
        "# Create DataLoader\n",
        "# dataloader = DataLoader(torch_dataset, batch_size=batch_size, num_workers=2)\n"
      ],
      "metadata": {
        "id": "a63ef7spFxvg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, PeftModel, PeftConfig, TaskType\n",
        "from functools import partial\n",
        "base_model = BlipForQuestionAnswering.from_pretrained(model_name, device_map=\"auto\")\n",
        "# PEFT Config\n",
        "peft_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"fc1\", \"fc2\"],\n",
        "    task_type=TaskType.QUESTION_ANS\n",
        ")\n",
        "if checkpoint_path is not None:\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    config = PeftConfig.from_pretrained(checkpoint_path)\n",
        "    model = PeftModel.from_pretrained(base_model, checkpoint_path, is_trainable=True)\n",
        "\n",
        "else:\n",
        "    print(\"Training from scratch\")\n",
        "    model = get_peft_model(base_model, peft_config)\n",
        "def new_forward(self, *args, **kwargs):\n",
        "    # Exclude 'inputs_embeds' from kwargs if present\n",
        "    kwargs.pop(\"inputs_embeds\", None)\n",
        "    return self.base_model.forward(*args, **kwargs)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Apply the modified forward method to your model\n",
        "model.forward = partial(new_forward, model)\n",
        "\n",
        "# for name, param in model.named_parameters():\n",
        "#     if \"lora\" in name:\n",
        "#         param.requires_grad = True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnZxELfzttNK",
        "outputId": "d277a1b6-4086-4159-b117-858c13eebf2f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint from /content/drive/MyDrive/images/lora_on_listing_0/checkpoint-11000\n",
            "trainable params: 2,506,752 || all params: 387,179,324 || trainable%: 0.6474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "args = TrainingArguments(\n",
        "      output_dir=output_dir,\n",
        "      per_device_train_batch_size=batch_size,\n",
        "      gradient_accumulation_steps= 1,\n",
        "      warmup_ratio=0.1,\n",
        "      save_strategy=\"steps\",\n",
        "      logging_steps=1000,\n",
        "      learning_rate=2e-5,\n",
        "      fp16=True,\n",
        "      max_steps=max_steps,\n",
        "      report_to=[],  # ✅ disables wandb, tensorboard, etc.\n",
        "      save_total_limit=2,\n",
        "      save_steps=1000,\n",
        "  )\n",
        "trainer = Trainer(\n",
        "      model=model,\n",
        "      args=args,\n",
        "      train_dataset=torch_dataset,\n",
        "      # data_collator=default_data_collator\n",
        "  )\n",
        "model.print_trainable_parameters()\n",
        "trainer.train()\n",
        "\n",
        "  # Save model and processor\n",
        "model.save_pretrained(output_dir)\n",
        "processor.save_pretrained(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "fwXzEVC3sB_p",
        "outputId": "6c6c5e7f-2bf7-4797-916f-12918c8dffec"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForQuestionAnswering`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2,506,752 || all params: 387,179,324 || trainable%: 0.6474\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9693' max='9693' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9693/9693 1:18:29, Epoch 1/9223372036854775807]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>7.796900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>7.793900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>7.785000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>7.785100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>7.783300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>7.780100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>7.772600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>7.772100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>7.774100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(output_dir)\n",
        "processor.save_pretrained(output_dir)\n",
        "print(f\"Model saved to {output_dir}\")\n"
      ],
      "metadata": {
        "id": "5XHlaJ0ekhz0",
        "outputId": "57a5ed06-68b2-4fc8-f321-a0b0638c3835",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/images/lora_on_listing_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qzy7Ro_p6xy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}