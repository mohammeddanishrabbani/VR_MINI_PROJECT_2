{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T18:55:05.147259Z",
     "iopub.status.busy": "2025-05-13T18:55:05.147089Z",
     "iopub.status.idle": "2025-05-13T18:55:18.713848Z",
     "shell.execute_reply": "2025-05-13T18:55:18.712936Z",
     "shell.execute_reply.started": "2025-05-13T18:55:05.147243Z"
    },
    "id": "8mnDnAg2NUoF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !uv pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !uv pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
    "    !uv pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
    "    !uv pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "z9_0IL7LQcRM",
    "outputId": "585a4e5b-66b9-45aa-909f-a2d4500ce79c",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T18:55:18.715048Z",
     "iopub.status.busy": "2025-05-13T18:55:18.714826Z",
     "iopub.status.idle": "2025-05-13T18:55:24.550537Z",
     "shell.execute_reply": "2025-05-13T18:55:24.549894Z",
     "shell.execute_reply.started": "2025-05-13T18:55:18.715025Z"
    },
    "id": "lVcieD3SQd_e",
    "outputId": "f9899c87-1673-4141-f96f-b00b4f764217",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 76074\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/danish/VR_PROJECT')\n",
    "import torch\n",
    "# ========= CONFIG =========\n",
    "listing = \"0\"\n",
    "csv_path = \"dataset/VQA-dataset-train/merged_listings.csv\"  # CSV must contain: image_path, question, one_word_answer\n",
    "#get length of csv filevvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
    "f = open(csv_path, \"r\")\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "\n",
    "model_dir = \"SFT/models\"\n",
    "output_dir = \"SFT/lora_on_listing\"\n",
    "images_root = \"dataset/abo-images-small/images/small\"\n",
    "batch_size = 16\n",
    "gradient_accumulation_steps = 1\n",
    "lenght_of_dataset = len(lines) -1\n",
    "max_steps = (len(lines)-1)//(batch_size * gradient_accumulation_steps)\n",
    "print(f\"Steps per epoch: {max_steps}\")\n",
    "checkpoint_path = None\n",
    "# ===========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "del lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xLDGk41C7IF"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T19:11:09.490233Z",
     "iopub.status.busy": "2025-05-13T19:11:09.489956Z",
     "iopub.status.idle": "2025-05-13T19:11:21.857754Z",
     "shell.execute_reply": "2025-05-13T19:11:21.857104Z",
     "shell.execute_reply.started": "2025-05-13T19:11:09.490213Z"
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "c4e79bcf-82a5-46ab-9a27-6f54d0ef7705",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danish/VR_PROJECT/.vr_project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 05-14 10:08:36 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 05-14 10:08:36 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-14 10:09:03,532\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.5.2: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2080 Ti. Num GPUs = 1. Max memory: 10.569 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
    "import torch\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T19:11:21.859018Z",
     "iopub.status.busy": "2025-05-13T19:11:21.858807Z",
     "iopub.status.idle": "2025-05-13T19:11:27.827802Z",
     "shell.execute_reply": "2025-05-13T19:11:27.827223Z",
     "shell.execute_reply.started": "2025-05-13T19:11:21.859001Z"
    },
    "id": "6bZsfBuZDeCL",
    "outputId": "6df28474-ae53-4d3d-d06d-b5f11b3abc24",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.visual` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 4,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 4,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:11:27.828728Z",
     "iopub.status.busy": "2025-05-13T19:11:27.828542Z",
     "iopub.status.idle": "2025-05-13T19:11:27.837029Z",
     "shell.execute_reply": "2025-05-13T19:11:27.836277Z",
     "shell.execute_reply.started": "2025-05-13T19:11:27.828714Z"
    },
    "id": "2V6E4SZwQy-Y",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "from PIL import Image\n",
    "class VQALazyIterableDataset(IterableDataset):\n",
    "    def __init__(self, dataset_stream, lenght_of_dataset,tokenizer):\n",
    "        self.dataset_stream = dataset_stream\n",
    "        self.lenght_of_dataset = lenght_of_dataset\n",
    "        self.images_root = \"dataset/abo-images-small/images/small\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def preprocess(self, example):\n",
    "        \n",
    "\n",
    "        image = Image.open(f\"{self.images_root}/{example['image_path']}\").convert(\"RGB\")\n",
    "        image = image.resize((224, 224))\n",
    "        question = example[\"question\"]\n",
    "        answer = example[\"answer\"]\n",
    "    \n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": question}\n",
    "            ]}\n",
    "        ]\n",
    "        input_text = self.tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "        inputs = self.tokenizer(\n",
    "            image,\n",
    "            input_text,\n",
    "            add_special_tokens = False,\n",
    "            max_length=300,\n",
    "            truncation=True,\n",
    "            return_tensors = \"pt\",\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "\n",
    "        labels = self.tokenizer.tokenizer(\n",
    "                text=answer,\n",
    "                add_special_tokens = False,\n",
    "                max_length=300,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors = \"pt\",\n",
    "            ).input_ids\n",
    "\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels.squeeze(0),\n",
    "            \"image_grid_thw\": inputs[\"image_grid_thw\"].squeeze(0)\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        for example in self.dataset_stream:\n",
    "            processed = self.preprocess(example)\n",
    "            if processed:\n",
    "                yield processed\n",
    "    def __len__(self):\n",
    "      return self.lenght_of_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:11:27.838587Z",
     "iopub.status.busy": "2025-05-13T19:11:27.838350Z",
     "iopub.status.idle": "2025-05-13T19:11:27.857464Z",
     "shell.execute_reply": "2025-05-13T19:11:27.856861Z",
     "shell.execute_reply.started": "2025-05-13T19:11:27.838572Z"
    },
    "id": "C9Rvo3FfiZO2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    import torch\n",
    "\n",
    "    # Extract fields from batch\n",
    "    pixel_values = [example[\"pixel_values\"] for example in batch]\n",
    "    input_ids = [example[\"input_ids\"] for example in batch]\n",
    "    attention_mask = [example[\"attention_mask\"] for example in batch]\n",
    "    labels = [example[\"labels\"] for example in batch]\n",
    "    image_grid_thw = [example[\"image_grid_thw\"] for example in batch]\n",
    "\n",
    "    # Stack vision-related tensors\n",
    "    pixel_values = torch.stack(pixel_values)         # [B, 256, 1176]\n",
    "    image_grid_thw = torch.stack(image_grid_thw)     # [B, 3]\n",
    "\n",
    "    # Tokenizer padding for input_ids & attention_mask\n",
    "    text_inputs = [\n",
    "        {\"input_ids\": ids, \"attention_mask\": mask}\n",
    "        for ids, mask in zip(input_ids, attention_mask)\n",
    "    ]\n",
    "    padded_inputs = tokenizer.tokenizer.pad(\n",
    "        text_inputs,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Tokenizer padding for labels\n",
    "    padded_labels = tokenizer.tokenizer.pad(\n",
    "        [{\"input_ids\": lbl} for lbl in labels],\n",
    "        return_tensors=\"pt\"\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    # Replace padding token id with -100 for loss masking\n",
    "    padded_labels[padded_labels == tokenizer.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\n",
    "        \"pixel_values\": pixel_values,                           # [B, 256, 1176]\n",
    "        \"input_ids\": padded_inputs[\"input_ids\"],                # [B, T]\n",
    "        \"attention_mask\": padded_inputs[\"attention_mask\"],      # [B, T]\n",
    "        \"labels\": padded_labels,                                # [B, T] with -100\n",
    "        \"image_grid_thw\": image_grid_thw                        # [B, 3]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:21:12.634818Z",
     "iopub.status.busy": "2025-05-13T19:21:12.634537Z",
     "iopub.status.idle": "2025-05-13T19:21:12.857321Z",
     "shell.execute_reply": "2025-05-13T19:21:12.856822Z",
     "shell.execute_reply.started": "2025-05-13T19:21:12.634797Z"
    },
    "id": "LjY75GoYUCB8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=csv_path, split=\"train\", streaming=True)\n",
    "dataset = dataset.filter(lambda x: x['image_path'] and x[\"question\"] and x[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1W2Qhsz6rUT"
   },
   "source": [
    "Let's take an overview look at the dataset. We shall see what the 3rd image is, and what caption it had."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LIksTxGsABV",
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T19:21:21.493836Z",
     "iopub.status.busy": "2025-05-13T19:21:21.493131Z",
     "iopub.status.idle": "2025-05-13T19:21:21.497278Z",
     "shell.execute_reply": "2025-05-13T19:21:21.496540Z",
     "shell.execute_reply.started": "2025-05-13T19:21:21.493811Z"
    },
    "id": "bfcSGwIb6p_R",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vqa_dataset = VQALazyIterableDataset(dataset,lenght_of_dataset, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T19:21:25.326382Z",
     "iopub.status.busy": "2025-05-13T19:21:25.326120Z",
     "iopub.status.idle": "2025-05-13T19:21:25.360344Z",
     "shell.execute_reply": "2025-05-13T19:21:25.359728Z",
     "shell.execute_reply.started": "2025-05-13T19:21:25.326363Z"
    },
    "id": "amkWVOXBtIwH",
    "outputId": "9bb5b230-9e39-4702-ed18-57f2657f339f",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([1, 256, 1176])\n",
      "input_ids torch.Size([1, 300])\n",
      "attention_mask torch.Size([1, 300])\n",
      "labels torch.Size([1, 300])\n",
      "image_grid_thw torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(\n",
    "    vqa_dataset,\n",
    "    batch_size=1,\n",
    ")\n",
    "batch = next(iter(dataloader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T19:21:29.241062Z",
     "iopub.status.busy": "2025-05-13T19:21:29.240514Z",
     "iopub.status.idle": "2025-05-13T19:21:29.291963Z",
     "shell.execute_reply": "2025-05-13T19:21:29.291355Z",
     "shell.execute_reply.started": "2025-05-13T19:21:29.241037Z"
    },
    "id": "95_Nn-89DhsL",
    "outputId": "dc1d8be2-ff56-4810-c6c4-ba346d34b263",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    # tokenizer = tokenizer,\n",
    "    # data_collator = collate_fn, # Must use!\n",
    "    train_dataset = vqa_dataset,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = batch_size,\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        logging_steps = 10,\n",
    "        save_steps=1000,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = output_dir,\n",
    "        report_to = \"none\",     # For Weights and Biases\n",
    "        save_total_limit=2,\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        max_steps = max_steps\n",
    "\n",
    "\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T19:11:30.563106Z",
     "iopub.status.busy": "2025-05-13T19:11:30.562524Z",
     "iopub.status.idle": "2025-05-13T19:11:30.568084Z",
     "shell.execute_reply": "2025-05-13T19:11:30.567476Z",
     "shell.execute_reply.started": "2025-05-13T19:11:30.563084Z"
    },
    "id": "2ejIt2xSNKKp",
    "outputId": "3dc5b8d8-39cc-4629-efd0-c42a218312f0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 2080 Ti. Max memory = 10.569 GB.\n",
      "2.391 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "execution": {
     "iopub.execute_input": "2025-05-13T19:21:41.251253Z",
     "iopub.status.busy": "2025-05-13T19:21:41.250787Z",
     "iopub.status.idle": "2025-05-13T19:30:38.562326Z",
     "shell.execute_reply": "2025-05-13T19:30:38.561182Z",
     "shell.execute_reply.started": "2025-05-13T19:21:41.251232Z"
    },
    "id": "yqxqAZ7KJ4oL",
    "outputId": "5e2144e5-a7a3-4d6e-d49d-ee99e58396af",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,217,194 | Num Epochs = 1 | Total steps = 76,074\n",
      "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 1 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 7,237,632/2,000,000,000 (0.36% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31' max='76074' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   31/76074 01:02 < 45:47:50, 0.46 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.849100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>7.848500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# checkpoint_path = '/content/outputs/checkpoint-30'\n",
    "\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-05-13T18:56:34.566971Z",
     "iopub.status.idle": "2025-05-13T18:56:34.567249Z",
     "shell.execute_reply": "2025-05-13T18:56:34.567144Z",
     "shell.execute_reply.started": "2025-05-13T18:56:34.567131Z"
    },
    "id": "upcOlWe7A1vc",
    "outputId": "298efe98-7038-4b57-fabc-695baf1e189e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(f\"{model_dir}/lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(f\"{model_dir}/lora_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7410323,
     "sourceId": 11800121,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 119203882,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".vr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
