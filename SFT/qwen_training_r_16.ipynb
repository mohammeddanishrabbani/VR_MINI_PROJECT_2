{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKEdcd7vpnoo",
        "outputId": "0481dcaa-ae12-4f7d-a9cd-7980a3be1e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.11.12 environment at: /usr\u001b[0m\n",
            "░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 0/0\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mxformers==0.0.29.post3                                                        \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mtrl==0.15.2                                                                   \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mbitsandbytes==0.45.5                                                          \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2maccelerate==1.6.0                                                             \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mpeft==0.15.2                                                                  \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mtriton==3.2.0                                                                 \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mcut-cross-entropy==25.1.1                                                     \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2munsloth-zoo==2025.5.7                                                         \u001b[0m\r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2m                                                                              \u001b[0m\r\u001b[2K\u001b[2mResolved \u001b[1m8 packages\u001b[0m \u001b[2min 69ms\u001b[0m\u001b[0m\n",
            "\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/0)                                                   \r\u001b[2K\u001b[37m⠋\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)                                                   \r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)                                                   \r\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/72.54 MiB                   \u001b[1A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/311.46 KiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/72.54 MiB                   \u001b[2A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 16.00 KiB/311.46 KiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/72.54 MiB                   \u001b[2A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 32.00 KiB/311.46 KiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/72.54 MiB                   \u001b[2A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 32.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/72.54 MiB                   \u001b[3A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[3A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 32.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/72.54 MiB                   \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 32.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/72.54 MiB                   \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 32.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m\u001b[2m------------------------------\u001b[0m\u001b[0m     0 B/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/72.54 MiB                 \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 32.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/72.54 MiB                 \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m----\u001b[2m--------------------------\u001b[0m\u001b[0m 14.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/72.54 MiB                 \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 30.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/72.54 MiB                 \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/72.54 MiB                 \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 32.00 KiB/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/72.54 MiB                 \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 41.77 KiB/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/72.54 MiB                 \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 57.77 KiB/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/72.54 MiB                 \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 73.77 KiB/41.35 MiB\n",
            "\u001b[2mbitsandbytes\u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 16.00 KiB/72.54 MiB                 \u001b[4A\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[1B\r\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 46.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 48.00 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-\u001b[2m-----------------------------\u001b[0m\u001b[0m 80.00 KiB/41.35 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mcut-cross-entropy\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.14 KiB\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 62.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 63.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--\u001b[2m----------------------------\u001b[0m\u001b[0m 1.98 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mcut-cross-entropy\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.14 KiB\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 62.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 63.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m---\u001b[2m---------------------------\u001b[0m\u001b[0m 3.89 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mcut-cross-entropy\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.14 KiB\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 94.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------\u001b[2m-----------------------\u001b[0m\u001b[0m 63.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-----\u001b[2m-------------------------\u001b[0m\u001b[0m 5.73 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mcut-cross-entropy\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.14 KiB\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 110.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 79.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------\u001b[2m------------------------\u001b[0m\u001b[0m 7.76 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mcut-cross-entropy\u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 16.00 KiB/22.14 KiB\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 110.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 79.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m---------\u001b[2m---------------------\u001b[0m\u001b[0m 11.47 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mcut-cross-entropy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 22.14 KiB/22.14 KiB\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 110.88 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 79.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m----------\u001b[2m--------------------\u001b[0m\u001b[0m 13.26 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mcut-cross-entropy\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 22.14 KiB/22.14 KiB\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 134.89 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 111.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 15.23 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[5A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 134.89 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 111.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------\u001b[2m------------------\u001b[0m\u001b[0m 15.50 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠹\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 134.89 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 127.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 17.31 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2munsloth-zoo\u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 134.89 KiB/134.89 KiB\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 127.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 19.42 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[4A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--------------\u001b[2m----------------\u001b[0m\u001b[0m 143.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 21.29 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 159.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 21.65 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠸\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 159.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 23.25 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----------------\u001b[2m--------------\u001b[0m\u001b[0m 159.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 25.25 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-----------------\u001b[2m-------------\u001b[0m\u001b[0m 175.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m---------------------\u001b[2m---------\u001b[0m\u001b[0m 28.09 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------------------\u001b[2m-----------\u001b[0m\u001b[0m 191.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m----------------------\u001b[2m--------\u001b[0m\u001b[0m 30.09 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m--------------------\u001b[2m----------\u001b[0m\u001b[0m 207.61 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-----------------------\u001b[2m-------\u001b[0m\u001b[0m 31.64 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠼\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 255.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m-------------------------\u001b[2m-----\u001b[0m\u001b[0m 33.16 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m---------------------------\u001b[2m---\u001b[0m\u001b[0m 271.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m--------------------------\u001b[2m----\u001b[0m\u001b[0m 35.00 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 287.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m----------------------------\u001b[2m--\u001b[0m\u001b[0m 37.94 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mtrl       \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 303.71 KiB/311.46 KiB\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 40.24 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[3A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 40.98 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠴\u001b[0m \u001b[2mPreparing packages...\u001b[0m (2/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 40.98 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 40.99 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 40.99 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 41.02 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠦\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 41.05 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 41.08 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 41.09 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 41.11 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠧\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\n",
            "\u001b[2mxformers  \u001b[0m \u001b[32m------------------------------\u001b[2m\u001b[0m\u001b[0m 41.12 MiB/41.35 MiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠇\u001b[0m \u001b[2mPreparing packages...\u001b[0m (3/5)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m5 packages\u001b[0m \u001b[2min 1.42s\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m5 packages\u001b[0m \u001b[2min 11ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.45.5\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mcut-cross-entropy\u001b[0m\u001b[2m==25.1.1\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.15.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1munsloth-zoo\u001b[0m\u001b[2m==2025.5.7\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mxformers\u001b[0m\u001b[2m==0.0.29.post3\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m34 packages\u001b[0m \u001b[2min 240ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 48.00 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m--------\u001b[2m----------------------\u001b[0m\u001b[0m 48.00 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 64.00 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 64.00 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 64.00 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m-----------\u001b[2m-------------------\u001b[0m\u001b[0m 64.00 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 77.39 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m-------------\u001b[2m-----------------\u001b[0m\u001b[0m 77.39 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 93.39 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 93.39 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 93.39 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m---------------\u001b[2m---------------\u001b[0m\u001b[0m 93.39 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2mfsspec    \u001b[0m \u001b[32m------------------\u001b[2m------------\u001b[0m\u001b[0m 109.39 KiB/189.08 KiB\n",
            "\u001b[2K\u001b[2A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/2)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 35ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 13ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 8ms\u001b[0m\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==2.14.4\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
            " \u001b[31m-\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.2\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.3.0\u001b[0m\n",
            "\u001b[2mUsing Python 3.11.12 environment at: /usr\u001b[0m\n",
            "\u001b[2K\u001b[2mResolved \u001b[1m1 package\u001b[0m \u001b[2min 15ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[1A\u001b[37m⠙\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)\n",
            "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 20ms\u001b[0m\u001b[0m\n",
            "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
            " \u001b[32m+\u001b[39m \u001b[1munsloth\u001b[0m\u001b[2m==2025.5.6\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !uv pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !uv pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !uv pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !uv pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n",
        "!tar -xf abo-images-small.tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYVg-5hMp8GQ",
        "outputId": "7f218b43-cc99-43df-a059-b7a3884b847f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-18 13:49:51--  https://amazon-berkeley-objects.s3.amazonaws.com/archives/abo-images-small.tar\n",
            "Resolving amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)... 52.216.37.177, 54.231.131.241, 52.216.32.33, ...\n",
            "Connecting to amazon-berkeley-objects.s3.amazonaws.com (amazon-berkeley-objects.s3.amazonaws.com)|52.216.37.177|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3253381120 (3.0G) [application/x-tar]\n",
            "Saving to: ‘abo-images-small.tar’\n",
            "\n",
            "abo-images-small.ta 100%[===================>]   3.03G  16.0MB/s    in 3m 22s  \n",
            "\n",
            "2025-05-18 13:53:14 (15.3 MB/s) - ‘abo-images-small.tar’ saved [3253381120/3253381120]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYV5CiR3qR7d",
        "outputId": "146ddf28-ba5b-4ac9-cd1e-30da4a073e2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.chdir('/workspace')\n",
        "import torch\n",
        "# ========= CONFIG =========\n",
        "listing = \"0\"\n",
        "csv_path = \"/content/drive/MyDrive/images/VQA_dataset_train/merged_listings.csv\"  # CSV must contain: image_path, question, one_word_answer\n",
        "#get length of csv filevvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n",
        "f = open(csv_path, \"r\")\n",
        "lines = f.readlines()\n",
        "f.close()"
      ],
      "metadata": {
        "id": "TpyNTE81p27q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/drive/MyDrive/images/model\"\n",
        "output_dir = \"/content/drive/MyDrive/images/lora_on_listing_for_qwen2_r_16\"\n",
        "images_root = \"dataset/abo-images-small/images/small\"\n",
        "batch_size = 32\n",
        "gradient_accumulation_steps = 1\n",
        "lenght_of_dataset = len(lines) -1\n",
        "max_steps = (len(lines)-1)//(batch_size * gradient_accumulation_steps)\n",
        "print(f\"Steps per epoch: {max_steps}\")\n",
        "checkpoint_path = None\n",
        "# ===========================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "del lines\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-hgjpYkqf1Y",
        "outputId": "654a38d2-e0a1-4809-bd0c-e890ae273b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steps per epoch: 38037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")\n",
        "\n",
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")\n",
        "\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from PIL import Image\n",
        "class VQALazyIterableDataset(IterableDataset):\n",
        "    def __init__(self, dataset_stream, lenght_of_dataset,tokenizer,padding_token_ids):\n",
        "        self.dataset_stream = dataset_stream\n",
        "        self.lenght_of_dataset = lenght_of_dataset\n",
        "        self.images_root = \"images/small\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.padding_token_ids =  padding_token_ids\n",
        "\n",
        "    def preprocess(self, example):\n",
        "\n",
        "\n",
        "        image = Image.open(f\"{self.images_root}/{example['image_path']}\").convert(\"RGB\")\n",
        "        image = image.resize((224, 224))\n",
        "        question = example[\"question\"]\n",
        "        answer = example[\"answer\"]\n",
        "\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\": f\"{question}. Answer with one word.\"}\n",
        "            ]},\n",
        "            {\"role\": \"assistant\", \"content\": [\n",
        "                {\"type\": \"text\", \"text\": f\"{answer}\"}\n",
        "            ]}\n",
        "        ]\n",
        "        input_text = self.tokenizer.apply_chat_template(messages, add_generation_prompt = False,tokenize = False)\n",
        "        inputs = self.tokenizer(\n",
        "            image,\n",
        "            input_text,\n",
        "            add_special_tokens = False,\n",
        "            max_length=300,\n",
        "            truncation=True,\n",
        "            return_tensors = \"pt\",\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "\n",
        "        labels = inputs[\"input_ids\"].clone()\n",
        "        labels[torch.isin(labels, self.padding_token_ids)] = -100\n",
        "\n",
        "\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": labels.squeeze(0),\n",
        "            \"image_grid_thw\": inputs[\"image_grid_thw\"].squeeze(0)\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        for example in self.dataset_stream:\n",
        "            processed = self.preprocess(example)\n",
        "            if processed:\n",
        "                yield processed\n",
        "    def __len__(self):\n",
        "      return self.lenght_of_dataset\n",
        "\n",
        "\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "data_collator = UnslothVisionDataCollator(model, tokenizer)\n",
        "def collate_fn(batch):\n",
        "    import torch\n",
        "\n",
        "    # Extract fields from batch\n",
        "    pixel_values = [example[\"pixel_values\"] for example in batch]\n",
        "    input_ids = [example[\"input_ids\"] for example in batch]\n",
        "    attention_mask = [example[\"attention_mask\"] for example in batch]\n",
        "    labels = [example[\"labels\"] for example in batch]\n",
        "    image_grid_thw = [example[\"image_grid_thw\"] for example in batch]\n",
        "\n",
        "    # Stack vision-related tensors\n",
        "    pixel_values = torch.stack(pixel_values)         # [B, 256, 1176]\n",
        "    image_grid_thw = torch.stack(image_grid_thw)     # [B, 3]\n",
        "\n",
        "    # Tokenizer padding for input_ids & attention_mask\n",
        "    text_inputs = [\n",
        "        {\"input_ids\": ids, \"attention_mask\": mask}\n",
        "        for ids, mask in zip(input_ids, attention_mask)\n",
        "    ]\n",
        "    padded_inputs = tokenizer.tokenizer.pad(\n",
        "        text_inputs,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Tokenizer padding for labels\n",
        "    padded_labels = tokenizer.tokenizer.pad(\n",
        "        [{\"input_ids\": lbl} for lbl in labels],\n",
        "        return_tensors=\"pt\"\n",
        "    )[\"input_ids\"]\n",
        "\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,                           # [B, 256, 1176]\n",
        "        \"input_ids\": padded_inputs[\"input_ids\"],                # [B, T]\n",
        "        \"attention_mask\": padded_inputs[\"attention_mask\"],      # [B, T]\n",
        "        \"labels\": padded_labels,                                # [B, T] with -100\n",
        "        \"image_grid_thw\": image_grid_thw                        # [B, 3]\n",
        "    }\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"csv\", data_files=csv_path, split=\"train\", streaming=True)\n",
        "dataset = dataset.filter(lambda x: x['image_path'] and x[\"question\"] and x[\"answer\"])\n",
        "\n",
        "\"\"\"Let's take an overview look at the dataset. We shall see what the 3rd image is, and what caption it had.\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "vqa_dataset = VQALazyIterableDataset(dataset,lenght_of_dataset, tokenizer, data_collator.padding_token_ids)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    vqa_dataset,\n",
        "    batch_size=1,\n",
        ")\n",
        "batch = next(iter(dataloader))\n",
        "for k, v in batch.items():\n",
        "    print(k, v.shape)\n",
        "\n",
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "FastVisionModel.for_training(model) # Enable for training!"
      ],
      "metadata": {
        "id": "d_elDRQ5POsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = collate_fn, # Must use!\n",
        "    train_dataset = vqa_dataset,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = batch_size,\n",
        "        gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bf16_supported(),\n",
        "        bf16 = is_bf16_supported(),\n",
        "        logging_steps = 32,\n",
        "        save_steps=32,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = output_dir,\n",
        "        report_to = \"none\",     # For Weights and Biases\n",
        "        save_total_limit=2,\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "        max_steps = max_steps\n",
        "\n",
        "\n",
        "    ),\n",
        ")\n",
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n",
        "\n",
        "checkpoint_path = '/content/drive/MyDrive/images/model/qwen-16/qwen-16/checkpoint-2720'\n",
        "\n",
        "trainer_stats = trainer.train(resume_from_checkpoint=checkpoint_path)\n",
        "\n",
        "model.save_pretrained(f\"{model_dir}/lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(f\"{model_dir}/lora_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4GDMj9JrMzi",
        "outputId": "83c863bb-287b-42d2-b059-7924dc139107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-2868d04dc2a9>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA L4. Max memory = 22.161 GB.\n",
            "6.844 GB of memory reserved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,217,194 | Num Epochs = 1 | Total steps = 38,037\n",
            "O^O/ \\_/ \\    Batch size per device = 32 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32\n",
            " \"-____-\"     Trainable parameters = 7,237,632/2,000,000,000 (0.36% trained)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bp1Hyn2YuDIA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}